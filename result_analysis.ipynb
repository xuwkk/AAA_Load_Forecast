{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the result and plotting the graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.model import DNN\n",
    "import yaml\n",
    "from utils.dataset import return_dataloader\n",
    "import numpy as np\n",
    "from algorithms.milp import MILP_Inte, MILP_Avai\n",
    "from attrdict import AttrDict\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cp\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "args = AttrDict({\n",
    "            \"flexible_feature\": np.arange(0,6),\n",
    "            \"fixed_feature\": np.arange(6,12),\n",
    "            \"batch_size\": 100,\n",
    "            \"impute_value\": 0.0          \n",
    "        })\n",
    "\n",
    "with open(\"utils/config.yml\", 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "train_loader, test_loader = return_dataloader(config, args)\n",
    "len(train_loader.dataset), len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot options\n",
    "plt.rcParams['lines.linewidth'] = 3.5\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "plt.rcParams['figure.figsize'] = (6,4)\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "plt.rcParams['legend.loc'] = 'lower right'\n",
    "plt.rcParams['legend.fontsize'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train size: ', len(train_loader.sampler), 'test size: ', len(test_loader.sampler))\n",
    "feature_size = train_loader.dataset[0][0].size(0)\n",
    "model_clean = DNN(feature_size, config['no_layer'], config['first_hidden_size'])\n",
    "model_clean.load_state_dict(torch.load('trained_models/dnn_model.pt'))\n",
    "\n",
    "model_adver_05 = DNN(feature_size, config['no_layer'], config['first_hidden_size'])\n",
    "model_adver_05.load_state_dict(torch.load('trained_models/dnn_model_adver_0.5.pt'))\n",
    "\n",
    "model_adver_00 = DNN(feature_size, config['no_layer'], config['first_hidden_size'])\n",
    "model_adver_00.load_state_dict(torch.load('trained_models/dnn_model_adver_0.0.pt'))\n",
    "model_clean.layer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_mape(model, loader):\n",
    "    mape_summary = []\n",
    "    for data, target in loader:\n",
    "        output = model(data)\n",
    "        mape = torch.abs((output - target) / target) * 100\n",
    "        mape_summary += mape.detach().numpy().tolist()\n",
    "    return np.mean(mape_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mape of the clean model\n",
    "print('Clean:')\n",
    "print('Clean Train', return_mape(model_clean, train_loader))\n",
    "print('Clean Test', return_mape(model_clean, test_loader))\n",
    "print('Adver 0.5:')\n",
    "print('Adver 0.5 Train', return_mape(model_adver_05, train_loader))\n",
    "print('Adver 0.5 Test', return_mape(model_adver_05, test_loader))\n",
    "print('Adver 0.0:')\n",
    "print('Adver 0.0 Train', return_mape(model_adver_00, train_loader))\n",
    "print('Adver 0.0 Test', return_mape(model_adver_00, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 0.\n",
    "idx = 0\n",
    "for mode in ['max', 'min']:\n",
    "    for i in range(1,7):\n",
    "        for impute_value in [0.0, 0.5]:\n",
    "            summary = np.load(f\"milp_result/plain_avai_{mode}_{i}_{impute_value}_True.npy\", allow_pickle=True).item()\n",
    "            time += summary['time']\n",
    "            idx += 1\n",
    "\n",
    "print('average time parallel: ', time / idx)\n",
    "\n",
    "time = 0.\n",
    "idx = 0\n",
    "for mode in ['max', 'min']:\n",
    "    for i in range(1,7):\n",
    "        for impute_value in [0.0, 0.5]:\n",
    "            summary = np.load(f\"milp_result/plain_avai_{mode}_{i}_{impute_value}_False.npy\", allow_pickle=True).item()\n",
    "            time += summary['time']\n",
    "            idx += 1\n",
    "\n",
    "print('average time sequential: ', time / idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Performance of the clean model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_avai(train_type, mode, impute_value):\n",
    "    time, dev, ave_dev, max_dev, min_dev, std_dev, vio_no, zero_no, success_rate, missing_no = [], [], [], [], [], [], [], [], [], []\n",
    "    for i in range(1,7):\n",
    "        summary = np.load(f\"milp_result/{train_type}_avai_{mode}_{i}_{impute_value}_True.npy\", allow_pickle=True).item()\n",
    "        time.append(round(summary['time'],2))\n",
    "        dev.append((summary['dev']))\n",
    "        ave_dev.append(round(summary['average deviation'],2))\n",
    "        max_dev.append(round(summary['max deviation'],2))\n",
    "        min_dev.append(round(summary['min deviation'],2))\n",
    "        std_dev.append(round(summary['std deviation'],2))\n",
    "        vio_no.append(summary['violation no'])\n",
    "        zero_no.append(round(summary['zero no']/len(test_loader.dataset)*100,2))\n",
    "        success_rate.append(summary['success rate'])\n",
    "        missing_no.append(summary['actual missing no'])\n",
    "        \n",
    "    return dev, time, missing_no, zero_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'max'\n",
    "impute_value = 0.0\n",
    "train_type = 'plain'\n",
    "dev, time, missing_no, zero_no = evaluate_avai(train_type, mode, impute_value)\n",
    "print('zero no:', zero_no)\n",
    "\n",
    "plt.boxplot(dev, widths = 0.5, showfliers=False, whis=(0,100))\n",
    "plt.xlabel(r'$\\beta$')\n",
    "plt.ylabel('MPE')\n",
    "if mode == 'max':\n",
    "    plt.ylim((-2, 35))\n",
    "else:\n",
    "    plt.ylim((-80, 2))\n",
    "    \n",
    "plt.savefig(f'figure/dev_{mode}_{impute_value}.pdf', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "for i in range(6):\n",
    "    plt.figure()\n",
    "    bins = np.arange(0, 6 + 1.5) - 0.5\n",
    "    plt.hist(missing_no[i], bins = bins)\n",
    "    plt.xticks(bins+0.5)\n",
    "    plt.xlim((-0.5, 6.5))\n",
    "    plt.ylim((0,len(test_loader.dataset)))\n",
    "    plt.xlabel('Actual Missing No.')\n",
    "    plt.ylabel('Data No.')\n",
    "    plt.savefig(f'figure/missing_{mode}_{impute_value}_{i}.pdf', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrity Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_inte(mode, epsilon):\n",
    "    summary = np.load(f'milp_result/plain_inte_{mode}_{epsilon}_True.npy', allow_pickle=True).item()\n",
    "    dev = summary['dev']\n",
    "    return dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'max'\n",
    "\n",
    "dev_avai_0, _, _, _ = evaluate_avai('plain', mode, 0.0)\n",
    "dev_avai_5, _, _, _ = evaluate_avai('plain', mode, 0.5)\n",
    "\n",
    "dev_inte = []\n",
    "for impute in [0.1,0.2,0.3,0.4,0.5]:\n",
    "    dev_inte.append(evaluate_inte(mode, impute))\n",
    "\n",
    "plt.boxplot(dev_inte, widths = 0.5, showfliers=False, whis=(0,100))\n",
    "plt.xlabel(r'$\\epsilon$')\n",
    "plt.xticks([1,2,3,4,5], [0.1, 0.2,0.3,0.4,0.5])\n",
    "plt.hlines(np.mean(dev_avai_0[-1]), 0, 6, colors='r', linestyles='dashed', label = f'AVAI({mode},0,6)')\n",
    "plt.hlines(np.mean(dev_avai_5[-1]), 0, 6, colors='g', linestyles='dashed', label = f'AVAI({mode},mean,6)')\n",
    "plt.legend(loc = 'best')\n",
    "plt.ylabel('MPE')\n",
    "plt.savefig(f'figure/inte_{mode}.pdf', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'min'\n",
    "impute_value = 0.0\n",
    "\n",
    "dev_clean, _, _, _ = evaluate_avai('plain', mode, impute_value)\n",
    "dev_adver_0, _, _, _ = evaluate_avai('0.0', mode, impute_value)\n",
    "dev_adver_5, _, _, _ = evaluate_avai('0.5', mode, impute_value)\n",
    "\n",
    "print('zero no:', zero_no)\n",
    "width = 0.3\n",
    "x = np.arange(1,7)\n",
    "\n",
    "plt.bar(x - 1.5*width, np.median(np.array(dev_clean), axis = -1), width, label = 'Clean')\n",
    "plt.bar(x - 0.5*width, np.median(np.array(dev_adver_0), axis = -1), width, label = 'Adver c = 0.0')\n",
    "plt.bar(x + 0.5*width, np.median(np.array(dev_adver_5), axis = -1), width, label = 'Adver c = mean')\n",
    "plt.xlabel(r'$\\beta$')\n",
    "plt.ylabel('MPE')\n",
    "plt.legend(loc = 'best')\n",
    "plt.savefig(f'figure/adver_{mode}_{impute_value}.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('load_forecasting')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 11:38:51) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c306fc40bb13b545c58ece5d4f8e36d926dbdb32f91d386d1513d8e02b0f83d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
